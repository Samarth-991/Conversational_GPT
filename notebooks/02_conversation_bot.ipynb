{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import json \n",
    "import pandas as pd \n",
    "import re\n",
    "excel_path = \"../data/Data_Mortgage.xlsx\"\n",
    "data_path = \"../data/Audio_data.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mortgage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(excel_path)\n",
    "excel_data.dropna(subset='Opportunity',inplace=True)\n",
    "excel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read The Json data of recordings\n",
    "with open(data_path,'r+') as fd:\n",
    "    audio_data = json.load(fd)\n",
    "fd.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the records for Sales Executive\n",
    "\n",
    "For example : ABC had converastions with multiple customers so giving this record to data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_executive = 'Juraira Manzoor'\n",
    "df_executive = excel_data[excel_data['Assigned']==name_of_executive]\n",
    "print(df_executive.shape)\n",
    "df_executive.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_dict = dict()\n",
    "\n",
    "conversation_dict['data']=list()\n",
    "Conversation = []\n",
    "customers = []\n",
    "\n",
    "for records in audio_data['Data']:\n",
    "    if records['content']['Assigned'] == name_of_executive:\n",
    "        text = re.sub(\"\\s\\s+\", \" \", records['content']['text'])\n",
    "        customer = records['content']['Customer']\n",
    "        text = \"conversation with Customer {}:{}\".format(customer,text)\n",
    "\n",
    "        conversation_dict['data'].append({\n",
    "                'name':name_of_executive,\n",
    "                'text':text,\n",
    "                'customer':records['content']['Customer'],\n",
    "                'date':''\n",
    "            })\n",
    "\n",
    "with open(\"../data/tmp.json\",'w+') as fc:\n",
    "    json.dump(conversation_dict,fc,indent=4)\n",
    "\n",
    "# with open('../data/tmp.txt','w+') as fp:\n",
    "#     for lines,name in zip(Conversation,customers):\n",
    "#         lines = re.sub(\"\\s\\s+\", \" \", lines)\n",
    "#         fp.writelines(\"Conversation with customer {} :{}\\n\".format(name,lines))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data as JsonLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader,TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata['customer'] = record.get('customer')\n",
    "    metadata['date'] = record.get('date')\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../data/tmp.json',\n",
    "    jq_schema='.data[]',\n",
    "    content_key=\"text\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "texts = loader.load()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "import openai\n",
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id): \n",
    "        # Should use the GPU by default\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using a locally running\n",
    "           Hugging Face Sentence Transformer model\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings =self.model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query using a locally running HF \n",
    "        Sentence trnsformer. \n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return list(map(float, embedding))\n",
    "\n",
    "# local_embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore =  FAISS.from_documents(documents=texts,\n",
    "                                    embedding = OpenAIEmbeddings()\n",
    "                                    ) # turn dcos into Vectors and store them in RAM also add metadata \n",
    "\n",
    "vectorstore.save_local('../data/faiss_dmac_gpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "chat = OpenAI(temperature=0)\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "vectordb = FAISS.load_local('../data/faiss_dmac_gpt/',embeddings=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=chat, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "doc_chain = load_qa_chain(chat,chain_type='map_reduce')\n",
    "\n",
    "chain = ConversationalRetrievalChain(retriever=vectordb.as_retriever(),question_generator=question_generator,\n",
    "                                           combine_docs_chain=doc_chain\n",
    "                                           )\n",
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:What is Majeed Interested in? \n",
      "response:\n",
      " Majeed is interested in taking a mortgage.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Majeed Interested in? \"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:How much mortage is he talking about ?\n",
      "response:\n",
      " Majeed is interested in taking a mortgage between 850,000 and 900,000.\n"
     ]
    }
   ],
   "source": [
    "query = \"How much mortage is he talking about ?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:What are the next steps for him ?\n",
      "response:\n",
      " Majeed's next steps regarding his mortgage are to provide the necessary documents to the mortgage advisor, who will then provide multiple offers from different banks for Majeed to choose from.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the next steps for him ?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:What is the value he looking for ?\n",
      "response:\n",
      " Majeed is interested in a mortgage value between 850,000 and 900,000.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the value he looking for ?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob \n",
    "\n",
    "# filenames = glob(\"../data/Processed_data/*.json\")\n",
    "\n",
    "\n",
    "# def merge_JsonFiles(filename):\n",
    "#     result = {'conversation data':[]}\n",
    "#     for f1 in filename:\n",
    "#         with open(f1, 'r') as infile:\n",
    "#             data = json.load(infile)\n",
    "        \n",
    "#         result['conversation data'].append(data['data'])\n",
    "\n",
    "#     with open('counseling3.json', 'w') as output_file:\n",
    "#         json.dump(result, output_file,indent=4)\n",
    "\n",
    "# merge_JsonFiles(filenames)\n",
    "# audio_data = \"../data/Audio_data_2.json\"\n",
    "# with open(audio_data,'r') as fd :\n",
    "#     data = json.load(fd)\n",
    "\n",
    "# conversation_dict = dict()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
