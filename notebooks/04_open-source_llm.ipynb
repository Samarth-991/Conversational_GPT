{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "repo_id =  \"tiiuae/falcon-7b\" # \"medmac01/moroccan-qa-falcon-7b-v3\"\n",
    "excel_path = \"../data/Data_Mortgage.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(huggingfacehub_api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.6, \"max_new_tokens\":250})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Company / Account</th>\n",
       "      <th>Opportunity</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Lead</th>\n",
       "      <th>Assigned</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Task</th>\n",
       "      <th>Ameyo Recording URL</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>CallDurationInSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/4/2023</td>\n",
       "      <td>Mohammed Jaffer</td>\n",
       "      <td>Mohammed Jaffer</td>\n",
       "      <td>Mohammed Jaffer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yaseen Syed Ali</td>\n",
       "      <td>Low</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>https://prypto-api.aswat.co/surveillance/recor...</td>\n",
       "      <td>Outbound</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/4/2023</td>\n",
       "      <td>G Abbas</td>\n",
       "      <td>G Abbas</td>\n",
       "      <td>G Abbas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yaseen Syed Ali</td>\n",
       "      <td>Low</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>https://prypto-api.aswat.co/surveillance/recor...</td>\n",
       "      <td>Outbound</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4/4/2023</td>\n",
       "      <td>Ahsan Khan</td>\n",
       "      <td>Ahsan Khan</td>\n",
       "      <td>Ahsan Khan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yaseen Syed Ali</td>\n",
       "      <td>Low</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>https://prypto-api.aswat.co/surveillance/recor...</td>\n",
       "      <td>Outbound</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4/5/2023</td>\n",
       "      <td>Fayiqa Iftikhar</td>\n",
       "      <td>Fayiqa Iftikhar</td>\n",
       "      <td>Fayiqa Iftikhar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yaseen Syed Ali</td>\n",
       "      <td>Low</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>https://prypto-api.aswat.co/surveillance/recor...</td>\n",
       "      <td>Outbound</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4/5/2023</td>\n",
       "      <td>Smith Suresh Shetty</td>\n",
       "      <td>Smith Suresh Shetty</td>\n",
       "      <td>Smith Suresh Shetty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yaseen Syed Ali</td>\n",
       "      <td>Low</td>\n",
       "      <td>Completed</td>\n",
       "      <td>True</td>\n",
       "      <td>https://prypto-api.aswat.co/surveillance/recor...</td>\n",
       "      <td>Outbound</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date    Company / Account          Opportunity           Unnamed: 3   \n",
       "1   4/4/2023      Mohammed Jaffer      Mohammed Jaffer      Mohammed Jaffer  \\\n",
       "2   4/4/2023              G Abbas              G Abbas              G Abbas   \n",
       "6   4/4/2023           Ahsan Khan           Ahsan Khan           Ahsan Khan   \n",
       "11  4/5/2023      Fayiqa Iftikhar      Fayiqa Iftikhar      Fayiqa Iftikhar   \n",
       "13  4/5/2023  Smith Suresh Shetty  Smith Suresh Shetty  Smith Suresh Shetty   \n",
       "\n",
       "   Lead         Assigned Priority     Status  Task   \n",
       "1   NaN  Yaseen Syed Ali      Low  Completed  True  \\\n",
       "2   NaN  Yaseen Syed Ali      Low  Completed  True   \n",
       "6   NaN  Yaseen Syed Ali      Low  Completed  True   \n",
       "11  NaN  Yaseen Syed Ali      Low  Completed  True   \n",
       "13  NaN  Yaseen Syed Ali      Low  Completed  True   \n",
       "\n",
       "                                  Ameyo Recording URL Call Type   \n",
       "1   https://prypto-api.aswat.co/surveillance/recor...  Outbound  \\\n",
       "2   https://prypto-api.aswat.co/surveillance/recor...  Outbound   \n",
       "6   https://prypto-api.aswat.co/surveillance/recor...  Outbound   \n",
       "11  https://prypto-api.aswat.co/surveillance/recor...  Outbound   \n",
       "13  https://prypto-api.aswat.co/surveillance/recor...  Outbound   \n",
       "\n",
       "    CallDurationInSeconds  \n",
       "1                     422  \n",
       "2                     237  \n",
       "6                      74  \n",
       "11                    481  \n",
       "13                    269  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "excel_data = pd.read_excel(excel_path)\n",
    "excel_data.dropna(subset='Opportunity',inplace=True)\n",
    "excel_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loader(tmp_path, chunk_size=1000, overlap=0):\n",
    "    def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "        metadata['customer'] = record.get('customer')\n",
    "        metadata['language'] = record.get('language')\n",
    "        metadata['duration'] = record.get('call duration')\n",
    "        return metadata\n",
    "\n",
    "    loader = JSONLoader(\n",
    "        file_path=tmp_path,\n",
    "        jq_schema='.data[]',\n",
    "        content_key=\"text\",\n",
    "        metadata_func=metadata_func\n",
    "    )\n",
    "    conversation_docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=overlap)\n",
    "    texts = text_splitter.split_documents(documents=conversation_docs)\n",
    "    return texts\n",
    "\n",
    "doc = data_loader(tmp_path=\"../data/Processed_data/Audio_data.json\",overlap=50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "class HuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id='multi-qa-mpnet-base-dot-v1'):\n",
    "          # Should use the GPU by default\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using a locally running\n",
    "           Hugging Face Sentence Transformer model\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query using a locally running HF\n",
    "        Sentence trnsformer.\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return list(map(float, embedding))\n",
    "    \n",
    "def save_to_local_vectorstore(docs, embedding):\n",
    "    vectorstore = None\n",
    "    try:\n",
    "        from langchain.vectorstores import FAISS\n",
    "        vectorstore = FAISS.from_documents(documents=docs, embedding=embedding, )\n",
    "    except ImportError as err:\n",
    "        raise (\"{} no module FAISS found. use pip install faiss\".format(err))\n",
    "    return vectorstore\n",
    "    \n",
    "huggingface_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vector Embeddings in FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if not os.path.isdir(\"../data/faiss_dmac_gpt_falcon'\"):\n",
    "    vectorstore =  FAISS.from_documents(documents=doc,\n",
    "                                    embedding = huggingface_embeddings\n",
    "                                    ) # turn dcos into Vectors and store them in RAM also add metadata \n",
    "    vectorstore.save_local('../data/faiss_dmac_gpt_falcon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def create_prompt():\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze conversations between customer and sales executive from context.\n",
    "    If customer shows interest in service or Property , conversation is a potential lead for business.  \n",
    "    Always answer point wise with person names. Don't make up answers\n",
    "   \n",
    "    {context}\n",
    "   \n",
    "    Question: {question}\n",
    "    Answer stepwise: \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "    return prompt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "vectordb = FAISS.load_local('../data/faiss_dmac_gpt_falcon/',embeddings=huggingface_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query With Falcon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain,RetrievalQA\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=create_prompt())\n",
    "doc_chain = load_qa_chain(llm,chain_type='map_reduce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "                                    llm = llm,\n",
    "                                    retriever=vectordb.as_retriever(),\n",
    "                                    # question_generator=question_generator,\n",
    "                                    chain_type_kwargs={\"prompt\": create_prompt()},\n",
    "                                    )\n",
    "chat_history = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUestion & Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:summarize provided conversations ? \n",
      "response:\n",
      " [email, Can I want. And he. Can you want. Yes, [email] [email]? Right. And go. The email. At. Okay.\n",
      "# # I don's. So, You. Send. Send. Send. (At the. This is it. Email. A Email. Email. Email. If you can write to email. I's the details, So. You email. It's. So, So. I's. So instead of. The rest of the of the same. You have you's. Is it. You can have you can. Please. Your name? 4- You's and. Please. I's. Thank you. The name. The other one. You can I's. Please give us. The name my name. So, you's.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"summarize provided conversations ? \"\n",
    "result = chain({\"query\": query})\n",
    "\n",
    "print(\"query:{}\".format(result['query']))\n",
    "print(\"response:\\n{}\".format(result['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:customers who are potential lead ? \n",
      "response:\n",
      " Hi. Talk to the bank. Hi. Have you have you are not. I's. Sir. Hi. Can you. . Good morning. Hi. I's and can. You would you have a mortgage, how is it. You. Or to you is it. I's. I am. Okay. I's. And you have you are mortgage you's. Please. Can I have you are you? And I's. I's. And I's you are you't 200, and. I's. I's the.\n",
      "\n",
      "This. Thank you. And I's. As I's. You's. Sir. Okay. Sir. And you's. And you. I's. Okay. Sir. You. And then. Or the house. You's. Sir. Your name? I's two names. And you. Come and on the key. And your leave the bank. I need you are you. The same. You's The. I need to the. Please. So the other , and I need to them. The time. I and I's. Okay. Meant\n"
     ]
    }
   ],
   "source": [
    "query = \"customers who are potential lead ? \"\n",
    "result = chain({\"query\": query, })\n",
    "\n",
    "print(\"query:{}\".format(result['query']))\n",
    "print(\"response:\\n{}\".format(result['result']))\n",
    "# chat_history.append((query,result['answer']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not working well , we might need to fine tune the model on the provided data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
