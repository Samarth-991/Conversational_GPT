{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b\"\n",
    "excel_path = \"../data/Data_Mortgage.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(huggingfacehub_api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.6, \"max_new_tokens\":500})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excel_data = pd.read_excel(excel_path)\n",
    "excel_data.dropna(subset='Opportunity',inplace=True)\n",
    "excel_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loader(tmp_path, chunk_size=1000, overlap=0):\n",
    "    def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "        metadata['customer'] = record.get('customer')\n",
    "        metadata['language'] = record.get('language')\n",
    "        metadata['duration'] = record.get('call duration')\n",
    "        return metadata\n",
    "\n",
    "    loader = JSONLoader(\n",
    "        file_path=tmp_path,\n",
    "        jq_schema='.data[]',\n",
    "        content_key=\"text\",\n",
    "        metadata_func=metadata_func\n",
    "    )\n",
    "    conversation_docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=overlap)\n",
    "    texts = text_splitter.split_documents(documents=conversation_docs)\n",
    "    return texts\n",
    "\n",
    "doc = data_loader(tmp_path=\"../data/Processed_data/Audio_data.json\",overlap=50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "class HuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id='multi-qa-mpnet-base-dot-v1'):\n",
    "          # Should use the GPU by default\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using a locally running\n",
    "           Hugging Face Sentence Transformer model\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query using a locally running HF\n",
    "        Sentence trnsformer.\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return list(map(float, embedding))\n",
    "    \n",
    "def save_to_local_vectorstore(docs, embedding):\n",
    "    vectorstore = None\n",
    "    try:\n",
    "        from langchain.vectorstores import FAISS\n",
    "        vectorstore = FAISS.from_documents(documents=docs, embedding=embedding, )\n",
    "    except ImportError as err:\n",
    "        raise (\"{} no module FAISS found. use pip install faiss\".format(err))\n",
    "    return vectorstore\n",
    "    \n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_id='all-mpnet-base-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Vector Embeddings in FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore =  FAISS.from_documents(documents=doc,\n",
    "                                    embedding = huggingface_embeddings\n",
    "                                    ) # turn dcos into Vectors and store them in RAM also add metadata \n",
    "vectorstore.save_local('../data/faiss_dmac_gpt_falcon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def create_prompt():\n",
    "    prompt_template = \"\"\"\n",
    "    Analyze conversations between customer and sales executive from context.\n",
    "    If customer shows interest in service or Property , conversation is a potential lead for business.  \n",
    "    Always answer point wise with person names. Don't make up answers\n",
    "   \n",
    "    {context}\n",
    "   \n",
    "    {chat_history}\n",
    "   \n",
    "    Question: {question}\n",
    "    Answer stepwise: \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\", \"chat_history\"], template=prompt_template)\n",
    "    return prompt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "vectordb = FAISS.load_local('../data/faiss_dmac_gpt_falcon/',embeddings=huggingface_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query With Falcon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=create_prompt())\n",
    "doc_chain = load_qa_chain(llm,chain_type='map_reduce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "                                    llm = llm,\n",
    "                                    retriever=vectordb.as_retriever(),\n",
    "                                    # question_generator=question_generator,\n",
    "                                    combine_docs_chain_kwargs={\"prompt\": create_prompt()},\n",
    "                                    )\n",
    "chat_history = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUestion & Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize conversations  ? \"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Names of customers who can be potential lead ? \"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"query:{}\".format(result['question']))\n",
    "print(\"response:\\n{}\".format(result['answer']))\n",
    "chat_history.append((query,result['answer']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
